{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import enviroments_package\n",
    "from enviroments_package import RemoveKeyObservationWrapper, ScaleRewardWrapper, ScaleActionWrapper\n",
    "import gymnasium\n",
    "\n",
    "from stable_baselines3.common.env_util import SubprocVecEnv\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import configure\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T18:54:34.532262200Z",
     "start_time": "2024-11-11T18:54:27.216705Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "#se crea una función para fabricar cada entorno con los wrapers correspondientes para un SubprocVecEnv\n",
    "def make_env():\n",
    "    def _init():\n",
    "        # Crea el entorno base\n",
    "        env = gymnasium.make('drone_tfg_juanes/Drone-v1', simulation_path=world_dir, reward_json_path=json_reward, no_render=False)\n",
    "\n",
    "        # Aplica los wrappers necesarios\n",
    "        env = RemoveKeyObservationWrapper(env, remove_keys=[\"camera\", \"gps\"])\n",
    "        env = ScaleRewardWrapper(env, scale_factor=0.1)\n",
    "        env = ScaleActionWrapper(env, in_low=-1, in_high=1, out_low=0, out_high=576)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "#creamos un callback para que el entorno funcione correctamente y no haya problemas con el entorno\n",
    "class TrainingCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=1):\n",
    "        super(TrainingCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_start(self) -> None:\n",
    "        self.env.reset()\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        print(\"Entrenamiento finalizado. Cerrando el entorno...\")\n",
    "        self.env.close()\n",
    "\n",
    "#aquí creamos un archivo para mover el progress.csv que geenra cada entrenamiento a la carpeta de data_collected\n",
    "def move_and_rename_csv(src_dir, dst_dir, new_name):\n",
    "    # Definir el archivo CSV específico a buscar\n",
    "    csv_file = 'progress.csv'\n",
    "    src_path = os.path.join(src_dir, csv_file)\n",
    "\n",
    "    # Verificar si el archivo 'progress.csv' existe en el directorio de origen\n",
    "    if not os.path.exists(src_path):\n",
    "        print(\"No se encontró el archivo 'progress.csv' en el directorio de origen.\")\n",
    "        return\n",
    "\n",
    "    # Definir la ruta de destino con el nuevo nombre\n",
    "    dst_path = os.path.join(dst_dir, new_name)\n",
    "\n",
    "    # Mover y renombrar el archivo\n",
    "    shutil.copy2(src_path, dst_path)\n",
    "    print(f\"Archivo copiado y renombrado a {dst_path}\")\n",
    "\n",
    "\n",
    "#creo una función para modificar el learning rate y el entropy coefficient\n",
    "def schedule_rate(initial_value, final_value, total_cycles, current_cycle):\n",
    "    return final_value if current_cycle >= total_cycles else initial_value + (final_value - initial_value) * (current_cycle / total_cycles)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T19:37:26.246510900Z",
     "start_time": "2024-11-11T19:37:26.235352600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "def update_model(model, env, log_dir='./logs', n_eval_episodes=10):\n",
    "    \"\"\"\n",
    "    Evalúa el rendimiento del modelo y actualiza un archivo de evaluación en CSV.\n",
    "\n",
    "    Args:\n",
    "        model: Modelo de Stable-Baselines3 a evaluar.\n",
    "        env: Entorno de entrenamiento.\n",
    "        log_dir (str): Carpeta para almacenar el archivo de evaluación.\n",
    "        n_eval_episodes (int): Número de episodios para evaluar el modelo.\n",
    "\n",
    "    Returns:\n",
    "        bool: True si el modelo supera la recompensa máxima registrada, False en caso contrario.\n",
    "    \"\"\"\n",
    "    eval_file_path = os.path.join(log_dir, \"evaluate.csv\")\n",
    "\n",
    "    # Crear el directorio y archivo si no existen\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    if not os.path.exists(eval_file_path):\n",
    "        # Crear archivo vacío con encabezado\n",
    "        pd.DataFrame(columns=[\"reward\", \"timestamp\"]).to_csv(eval_file_path, index=False)\n",
    "\n",
    "    # Leer el archivo de evaluación\n",
    "    eval_df = pd.read_csv(eval_file_path)\n",
    "\n",
    "    # Comprobar si el archivo tiene datos\n",
    "    if not eval_df.empty:\n",
    "        max_reward = eval_df[\"reward\"].max()\n",
    "    else:\n",
    "        max_reward = float('-inf')\n",
    "\n",
    "    # Evaluar el modelo en el entorno dado\n",
    "    mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=n_eval_episodes, return_episode_rewards=False)\n",
    "\n",
    "    # Comparar la recompensa y actualizar el archivo si es necesario\n",
    "    if mean_reward > max_reward:\n",
    "        # Registrar la nueva recompensa y timestamp en el archivo CSV\n",
    "        new_row = pd.DataFrame({\n",
    "            \"reward\": [mean_reward],\n",
    "            \"timestamp\": [datetime.now().strftime(\"%Y%m%d_%H%M%S\")]\n",
    "        })\n",
    "        new_row.to_csv(eval_file_path, mode='a', header=False, index=False)\n",
    "        result = True\n",
    "    else:\n",
    "        result = False\n",
    "    env.close()\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T18:54:34.563257100Z",
     "start_time": "2024-11-11T18:54:34.552262600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#aquí están los archivos del simulador y la configuración de la recompensa\n",
    "world_dir = \"/Users/jeste/Desktop/Clase/TFG/drone_tfg_juanes/simulation_package/worlds/my_frst_webots_world.wbt\"\n",
    "json_reward = \"/Users/jeste/Desktop/Clase/TFG/drone_tfg_juanes/configs/reward_package_config/motors_use.json\"\n",
    "# Define el número de entornos que se van a crear\n",
    "num_envs = 4\n",
    "\n",
    "#direcciones de dónde se guardará cada componente iportante del modelo\n",
    "model_dir = \"./models/ppomodel\"\n",
    "log_dir = \"./logs/\"\n",
    "data_collected_dir = './data_collected/'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "#los valores de learning rate y entropy coefficient\n",
    "lr = 1e-3\n",
    "ent_coef = 0.1\n",
    "\n",
    "#define los pasos totales que se usarán para entrenar al modelo en cada ciclo\n",
    "timesteps = 20480\n",
    "#define los pasos que se usrán antes de actualizar los pesos del modelo\n",
    "n_steps = 1024\n",
    "#define los paquetes de experiencia que se usarán para actualizar los pesos\n",
    "batch_size = 64\n",
    "\n",
    "#ciclos\n",
    "n_cycles = 2\n",
    "timeout_threshold = 240\n",
    "\n",
    "#creamos el entorno\n",
    "env = SubprocVecEnv([make_env() for _ in range(num_envs)])\n",
    "env = VecMonitor(env)#, filename=f'./data_collected/ppo_monitor{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T18:54:43.978512600Z",
     "start_time": "2024-11-11T18:54:34.567257500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/\n",
      "retrainning\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12.9     |\n",
      "|    ep_rew_mean     | 6.8      |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 392      |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 14.6      |\n",
      "|    ep_rew_mean          | 7.76      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 9         |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 842       |\n",
      "|    total_timesteps      | 8192      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0262491 |\n",
      "|    clip_fraction        | 0.252     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -7.32     |\n",
      "|    explained_variance   | 0.586     |\n",
      "|    learning_rate        | 0.001     |\n",
      "|    loss                 | -0.409    |\n",
      "|    n_updates            | 60        |\n",
      "|    policy_gradient_loss | -0.00595  |\n",
      "|    std                  | 1.53      |\n",
      "|    value_loss           | 2.16      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 15.6       |\n",
      "|    ep_rew_mean          | 8.47       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 9          |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 1284       |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02737476 |\n",
      "|    clip_fraction        | 0.236      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -7.61      |\n",
      "|    explained_variance   | 0.513      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 0.517      |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.00262   |\n",
      "|    std                  | 1.65       |\n",
      "|    value_loss           | 3.2        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 21          |\n",
      "|    ep_rew_mean          | 11.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 1707        |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021347608 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.86       |\n",
      "|    explained_variance   | 0.407       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.0545      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00339    |\n",
      "|    std                  | 1.75        |\n",
      "|    value_loss           | 4.69        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 21.8       |\n",
      "|    ep_rew_mean          | 12.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 9          |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 2144       |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02183255 |\n",
      "|    clip_fraction        | 0.193      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -8.09      |\n",
      "|    explained_variance   | 0.401      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 0.591      |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.00229   |\n",
      "|    std                  | 1.86       |\n",
      "|    value_loss           | 6.12       |\n",
      "----------------------------------------\n",
      "Entrenamiento finalizado. Cerrando el entorno...\n",
      "Archivo copiado y renombrado a ./data_collected/ppo_data20241111_203230.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeste\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEBOTS Result: INFO: xyz_controller: Starting controller: python.exe -u xyz_controller.py\n",
      "Simulation Starting\n",
      "RESET \n",
      "SET_ALL_MOTORS [227.20082 180.52658 101.46528 115.27259]\n",
      "SET_ALL_MOTORS [221.20131  175.14601   93.88118  107.961685]\n",
      "SET_ALL_MOTORS [215.64673 171.32141  88.88364 103.93745]\n",
      "SET_ALL_MOTORS [210.87537 168.29091  85.09205 101.29978]\n",
      "SET_ALL_MOTORS [206.68198 165.75739  82.01155  99.37503]\n",
      "SET_ALL_MOTORS [202.9137   163.56879   79.3921    97.862076]\n",
      "SET_ALL_MOTORS [199.48552  161.6471    77.10277   96.627174]\n",
      "SET_ALL_MOTORS [196.36661  159.96825   75.14143   95.703705]\n",
      "SET_ALL_MOTORS [193.4288  158.41472  73.24729  94.72788]\n",
      "SET_ALL_MOTORS [190.70859 157.03717  71.56137  93.9563 ]\n",
      "SET_ALL_MOTORS [188.16335 155.79091  70.01961  93.29279]\n",
      "SET_ALL_MOTORS [185.77582 154.65753  68.59556  92.71522]\n",
      "SET_ALL_MOTORS [183.52757 153.62389  67.27878  92.21719]\n",
      "RESET \n",
      "SET_ALL_MOTORS [227.20082 180.52658 101.4653  115.27259]\n",
      "SET_ALL_MOTORS [221.16655  175.11267   93.796326 107.85057 ]\n",
      "SET_ALL_MOTORS [215.61241  171.31177   88.86899  103.925446]\n",
      "SET_ALL_MOTORS [209.31918  167.79156   86.10794  102.118454]\n",
      "RESET \n",
      "SET_ALL_MOTORS [178.20999 188.81293 189.46394 170.00323]\n",
      "SET_ALL_MOTORS [215.42657 173.019    79.59916 104.29275]\n",
      "SET_ALL_MOTORS [191.64726  173.1528    81.977615 105.19147 ]\n",
      "SET_ALL_MOTORS [184.55864 168.54895  80.2171  104.5203 ]\n",
      "RESET \n",
      "SET_ALL_MOTORS [227.20082 180.52658 101.4653  115.27259]\n",
      "SET_ALL_MOTORS [221.16647 175.11263  93.79627 107.85048]\n",
      "SET_ALL_MOTORS [215.61795  171.31294   88.86506  103.922806]\n",
      "SET_ALL_MOTORS [210.83577 168.28223  85.10382 101.31731]\n",
      "RESET \n",
      "SET_ALL_MOTORS [146.6471  127.18326 220.81558  46.73658]\n",
      "SET_ALL_MOTORS [322.87357 379.145   178.95009 356.4804 ]\n",
      "SET_ALL_MOTORS [  3.78479 355.39716 280.55066 373.00226]\n",
      "SET_ALL_MOTORS [ 44.51542 281.9647  208.50153 322.9828 ]\n",
      "RESET \n",
      "SET_ALL_MOTORS [227.20084 180.52658 101.46528 115.27259]\n",
      "SET_ALL_MOTORS [221.16653 175.11267  93.79634 107.85057]\n",
      "SET_ALL_MOTORS [215.61246 171.31174  88.86894 103.92543]\n",
      "SET_ALL_MOTORS [201.48822  167.01999   97.473465 112.20521 ]\n",
      "RESET \n",
      "SET_ALL_MOTORS [439.8864  110.25118  85.56482 139.54408]\n",
      "SET_ALL_MOTORS [423.17505 185.4231  134.9154  269.52945]\n",
      "SET_ALL_MOTORS [370.9641 193.6298 163.5105 287.9348]\n",
      "SET_ALL_MOTORS [352.43475 189.68588 150.21788 263.50082]\n",
      "RESET \n",
      "SET_ALL_MOTORS [165.77919 135.85634 215.17523 185.82336]\n",
      "SET_ALL_MOTORS [210.32188 168.07999  76.75123  95.86622]\n",
      "SET_ALL_MOTORS [193.92665  170.86583   75.476105  92.90602 ]\n",
      "SET_ALL_MOTORS [185.66856 167.12323  77.08295  95.04676]\n",
      "RESET \n",
      "SET_ALL_MOTORS [  0.       307.39294   19.610338 217.92082 ]\n",
      "SET_ALL_MOTORS [ 12.676249  28.059305 257.96555  398.7428  ]\n",
      "SET_ALL_MOTORS [538.0324    24.563576   0.        18.100061]\n",
      "SET_ALL_MOTORS [498.73975   73.27356    0.        27.939657]\n",
      "SET_ALL_MOTORS [456.65112   84.48654    0.        37.574425]\n",
      "SET_ALL_MOTORS [417.1994    83.90344    0.        41.800148]\n",
      "RESET \n",
      "SET_ALL_MOTORS [  1.4357586 466.67297    41.280563  197.05739  ]\n",
      "SET_ALL_MOTORS [ 35.13594 271.5802  433.94788 500.21036]\n",
      "SET_ALL_MOTORS [531.8223  147.86932   0.        5.1089 ]\n",
      "SET_ALL_MOTORS [494.26968 142.62445   0.        0.     ]\n",
      "RESET \n",
      "CLOSE_CONNECTION \n",
      "INFO: xyz_controller: Terminating.\n",
      "\n",
      "Result Code: 0\n"
     ]
    }
   ],
   "source": [
    "new_logger = configure(log_dir, [\"stdout\", \"csv\"])\n",
    "callback = TrainingCallback(env=env, verbose=1)\n",
    "\n",
    "if not os.path.exists(model_dir+\".zip\"):\n",
    "    print(\"first train\")\n",
    "    model = RecurrentPPO(\n",
    "        \"MultiInputLstmPolicy\",\n",
    "        env,\n",
    "        verbose=1,                    # Si quiero ver las acciones por terminal\n",
    "        n_steps=n_steps,              # Controla el buffer de experiencias para actualizar la política\n",
    "        batch_size=batch_size,        # Tamaño del lote, separa el buffer de experiencias en paquetes de este tamaño\n",
    "        learning_rate=lr,     # Tasa de aprendizaje\n",
    "        ent_coef=ent_coef     # Coeficiente de entropía para exploración\n",
    "    )\n",
    "    model.set_logger(new_logger)\n",
    "    model.learn(total_timesteps=timesteps, callback=callback)\n",
    "    model.save(model_dir)\n",
    "\n",
    "    move_and_rename_csv(log_dir, data_collected_dir, f'ppo_data{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv')\n",
    "else:\n",
    "    print(\"retrainning\")\n",
    "    model = RecurrentPPO.load(model_dir+\".zip\", env=env)\n",
    "    model.set_logger(new_logger)\n",
    "\n",
    "    model.learning_rate = lr\n",
    "    model.ent_coef = ent_coef\n",
    "\n",
    "    model.learn(total_timesteps=timesteps, callback=callback)\n",
    "    time.sleep(5)\n",
    "    move_and_rename_csv(log_dir, data_collected_dir, f'ppo_data{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv')\n",
    "\n",
    "    if update_model(model, make_env()(), n_eval_episodes=10):\n",
    "        model.save(path=model_dir)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T19:33:45.956590200Z",
     "start_time": "2024-11-11T18:54:43.983513800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 8>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      6\u001B[0m observation, _ \u001B[38;5;241m=\u001B[39m show_env\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m100\u001B[39m):\n\u001B[1;32m----> 9\u001B[0m     action, _states \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdeterministic\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m     observation, reward, terminated, truncated, _ \u001B[38;5;241m=\u001B[39m show_env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m terminated:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:556\u001B[0m, in \u001B[0;36mBaseAlgorithm.predict\u001B[1;34m(self, observation, state, episode_start, deterministic)\u001B[0m\n\u001B[0;32m    536\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\n\u001B[0;32m    537\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    538\u001B[0m     observation: Union[np\u001B[38;5;241m.\u001B[39mndarray, Dict[\u001B[38;5;28mstr\u001B[39m, np\u001B[38;5;241m.\u001B[39mndarray]],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    541\u001B[0m     deterministic: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    542\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[np\u001B[38;5;241m.\u001B[39mndarray, Optional[Tuple[np\u001B[38;5;241m.\u001B[39mndarray, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m]]]:\n\u001B[0;32m    543\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    544\u001B[0m \u001B[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001B[39;00m\n\u001B[0;32m    545\u001B[0m \u001B[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    554\u001B[0m \u001B[38;5;124;03m        (used in recurrent policies)\u001B[39;00m\n\u001B[0;32m    555\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 556\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepisode_start\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdeterministic\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sb3_contrib\\common\\recurrent\\policies.py:410\u001B[0m, in \u001B[0;36mRecurrentActorCriticPolicy.predict\u001B[1;34m(self, observation, state, episode_start, deterministic)\u001B[0m\n\u001B[0;32m    406\u001B[0m     states \u001B[38;5;241m=\u001B[39m th\u001B[38;5;241m.\u001B[39mtensor(state[\u001B[38;5;241m0\u001B[39m], dtype\u001B[38;5;241m=\u001B[39mth\u001B[38;5;241m.\u001B[39mfloat32, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice), th\u001B[38;5;241m.\u001B[39mtensor(\n\u001B[0;32m    407\u001B[0m         state[\u001B[38;5;241m1\u001B[39m], dtype\u001B[38;5;241m=\u001B[39mth\u001B[38;5;241m.\u001B[39mfloat32, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice\n\u001B[0;32m    408\u001B[0m     )\n\u001B[0;32m    409\u001B[0m     episode_starts \u001B[38;5;241m=\u001B[39m th\u001B[38;5;241m.\u001B[39mtensor(episode_start, dtype\u001B[38;5;241m=\u001B[39mth\u001B[38;5;241m.\u001B[39mfloat32, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m--> 410\u001B[0m     actions, states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_predict\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    411\u001B[0m \u001B[43m        \u001B[49m\u001B[43mobservation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlstm_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstates\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepisode_starts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepisode_starts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdeterministic\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdeterministic\u001B[49m\n\u001B[0;32m    412\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    413\u001B[0m     states \u001B[38;5;241m=\u001B[39m (states[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy(), states[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy())\n\u001B[0;32m    415\u001B[0m \u001B[38;5;66;03m# Convert to numpy\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sb3_contrib\\common\\recurrent\\policies.py:364\u001B[0m, in \u001B[0;36mRecurrentActorCriticPolicy._predict\u001B[1;34m(self, observation, lstm_states, episode_starts, deterministic)\u001B[0m\n\u001B[0;32m    347\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_predict\u001B[39m(\n\u001B[0;32m    348\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    349\u001B[0m     observation: th\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    352\u001B[0m     deterministic: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    353\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[th\u001B[38;5;241m.\u001B[39mTensor, Tuple[th\u001B[38;5;241m.\u001B[39mTensor, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m]]:\n\u001B[0;32m    354\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    355\u001B[0m \u001B[38;5;124;03m    Get the action according to the policy for a given observation.\u001B[39;00m\n\u001B[0;32m    356\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    362\u001B[0m \u001B[38;5;124;03m    :return: Taken action according to the policy and hidden states of the RNN\u001B[39;00m\n\u001B[0;32m    363\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 364\u001B[0m     distribution, lstm_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_distribution\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlstm_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepisode_starts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    365\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m distribution\u001B[38;5;241m.\u001B[39mget_actions(deterministic\u001B[38;5;241m=\u001B[39mdeterministic), lstm_states\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sb3_contrib\\common\\recurrent\\policies.py:278\u001B[0m, in \u001B[0;36mRecurrentActorCriticPolicy.get_distribution\u001B[1;34m(self, obs, lstm_states, episode_starts)\u001B[0m\n\u001B[0;32m    276\u001B[0m latent_pi, lstm_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_sequence(features, lstm_states, episode_starts, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlstm_actor)\n\u001B[0;32m    277\u001B[0m latent_pi \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp_extractor\u001B[38;5;241m.\u001B[39mforward_actor(latent_pi)\n\u001B[1;32m--> 278\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_action_dist_from_latent\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlatent_pi\u001B[49m\u001B[43m)\u001B[49m, lstm_states\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\policies.py:694\u001B[0m, in \u001B[0;36mActorCriticPolicy._get_action_dist_from_latent\u001B[1;34m(self, latent_pi)\u001B[0m\n\u001B[0;32m    691\u001B[0m mean_actions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_net(latent_pi)\n\u001B[0;32m    693\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_dist, DiagGaussianDistribution):\n\u001B[1;32m--> 694\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maction_dist\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproba_distribution\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmean_actions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlog_std\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    695\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_dist, CategoricalDistribution):\n\u001B[0;32m    696\u001B[0m     \u001B[38;5;66;03m# Here mean_actions are the logits before the softmax\u001B[39;00m\n\u001B[0;32m    697\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_dist\u001B[38;5;241m.\u001B[39mproba_distribution(action_logits\u001B[38;5;241m=\u001B[39mmean_actions)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\distributions.py:164\u001B[0m, in \u001B[0;36mDiagGaussianDistribution.proba_distribution\u001B[1;34m(self, mean_actions, log_std)\u001B[0m\n\u001B[0;32m    156\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;124;03mCreate the distribution given its parameters (mean, std)\u001B[39;00m\n\u001B[0;32m    158\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    161\u001B[0m \u001B[38;5;124;03m:return:\u001B[39;00m\n\u001B[0;32m    162\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    163\u001B[0m action_std \u001B[38;5;241m=\u001B[39m th\u001B[38;5;241m.\u001B[39mones_like(mean_actions) \u001B[38;5;241m*\u001B[39m log_std\u001B[38;5;241m.\u001B[39mexp()\n\u001B[1;32m--> 164\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistribution \u001B[38;5;241m=\u001B[39m \u001B[43mNormal\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmean_actions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction_std\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\distributions\\normal.py:57\u001B[0m, in \u001B[0;36mNormal.__init__\u001B[1;34m(self, loc, scale, validate_args)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     56\u001B[0m     batch_shape \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloc\u001B[38;5;241m.\u001B[39msize()\n\u001B[1;32m---> 57\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mbatch_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidate_args\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidate_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\distributions\\distribution.py:69\u001B[0m, in \u001B[0;36mDistribution.__init__\u001B[1;34m(self, batch_shape, event_shape, validate_args)\u001B[0m\n\u001B[0;32m     67\u001B[0m         value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, param)\n\u001B[0;32m     68\u001B[0m         valid \u001B[38;5;241m=\u001B[39m constraint\u001B[38;5;241m.\u001B[39mcheck(value)\n\u001B[1;32m---> 69\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m valid\u001B[38;5;241m.\u001B[39mall():\n\u001B[0;32m     70\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m     71\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected parameter \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     72\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(value)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m of shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtuple\u001B[39m(value\u001B[38;5;241m.\u001B[39mshape)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     75\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut found invalid values:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mvalue\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     76\u001B[0m             )\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "init = make_env()\n",
    "show_env = init()\n",
    "\n",
    "model = RecurrentPPO.load(\"models/ppomodel\", env=env)\n",
    "\n",
    "observation, _ = show_env.reset()\n",
    "\n",
    "for i in range(100):\n",
    "    action, _states = model.predict(observation, deterministic=True)\n",
    "    observation, reward, terminated, truncated, _ = show_env.step(action)\n",
    "\n",
    "    if terminated:\n",
    "        observation, _ = show_env.reset()\n",
    "\n",
    "show_env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T19:34:44.236539200Z",
     "start_time": "2024-11-11T19:33:45.961741600Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
