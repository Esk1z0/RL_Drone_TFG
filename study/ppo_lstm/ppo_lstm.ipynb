{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import enviroments_package\n",
    "from enviroments_package import RemoveKeyObservationWrapper, ScaleRewardWrapper, ScaleActionWrapper, BinaryActionWrapper\n",
    "import gymnasium\n",
    "\n",
    "from stable_baselines3.common.env_util import SubprocVecEnv\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import configure\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-24T12:24:49.784267800Z",
     "start_time": "2024-12-24T12:24:41.756097800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#se crea una función para fabricar cada entorno con los wrapers correspondientes para un SubprocVecEnv\n",
    "def make_env():\n",
    "    def _init():\n",
    "        # Crea el entorno base\n",
    "        env = gymnasium.make('drone_tfg_juanes/Drone-v1', simulation_path=world_dir, reward_json_path=json_reward, no_render=False)\n",
    "\n",
    "        # Aplica los wrappers necesarios\n",
    "        env = RemoveKeyObservationWrapper(env, remove_keys=[\"camera\", \"gps\"])\n",
    "        env = ScaleRewardWrapper(env, scale_factor=0.1)\n",
    "        env = ScaleActionWrapper(env, in_low=-1, in_high=1, out_low=0, out_high=576)\n",
    "        #env = BinaryActionWrapper(env, power_level=500)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "#creamos un callback para que el entorno funcione correctamente y no haya problemas con el entorno\n",
    "class TrainingCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=1):\n",
    "        super(TrainingCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_start(self) -> None:\n",
    "        self.env.reset()\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        print(\"Entrenamiento finalizado. Cerrando el entorno...\")\n",
    "        self.env.close()\n",
    "\n",
    "#aquí creamos un archivo para mover el progress.csv que geenra cada entrenamiento a la carpeta de data_collected\n",
    "def move_and_rename_csv(src_dir, dst_dir, new_name):\n",
    "    # Definir el archivo CSV específico a buscar\n",
    "    csv_file = 'progress.csv'\n",
    "    src_path = os.path.join(src_dir, csv_file)\n",
    "\n",
    "    # Verificar si el archivo 'progress.csv' existe en el directorio de origen\n",
    "    if not os.path.exists(src_path):\n",
    "        print(\"No se encontró el archivo 'progress.csv' en el directorio de origen.\")\n",
    "        return\n",
    "\n",
    "    # Definir la ruta de destino con el nuevo nombre\n",
    "    dst_path = os.path.join(dst_dir, new_name)\n",
    "\n",
    "    # Mover y renombrar el archivo\n",
    "    shutil.copy2(src_path, dst_path)\n",
    "    print(f\"Archivo copiado y renombrado a {dst_path}\")\n",
    "\n",
    "\n",
    "#creo una función para modificar el learning rate y el entropy coefficient\n",
    "def schedule_rate(initial_value, final_value, total_cycles, current_cycle):\n",
    "    return final_value if current_cycle >= total_cycles else initial_value + (final_value - initial_value) * (current_cycle / total_cycles)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-24T12:24:49.798340600Z",
     "start_time": "2024-12-24T12:24:49.791271Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "def update_model(model, env, log_dir='./logs/', n_eval_episodes=10):\n",
    "    \"\"\"\n",
    "    Evalúa el rendimiento del modelo y actualiza un archivo de evaluación en CSV.\n",
    "\n",
    "    Args:\n",
    "        model: Modelo de Stable-Baselines3 a evaluar.\n",
    "        env: Entorno de entrenamiento.\n",
    "        log_dir (str): Carpeta para almacenar el archivo de evaluación.\n",
    "        n_eval_episodes (int): Número de episodios para evaluar el modelo.\n",
    "\n",
    "    Returns:\n",
    "        bool: True si el modelo supera la recompensa máxima registrada, False en caso contrario.\n",
    "    \"\"\"\n",
    "    eval_file_path = os.path.join(log_dir, \"evaluate.csv\")\n",
    "\n",
    "    # Crear el directorio y archivo si no existen\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    if not os.path.exists(eval_file_path):\n",
    "        # Crear archivo vacío con encabezado\n",
    "        pd.DataFrame(columns=[\"reward\", \"timestamp\"]).to_csv(eval_file_path, index=False)\n",
    "\n",
    "    # Leer el archivo de evaluación\n",
    "    eval_df = pd.read_csv(eval_file_path)\n",
    "\n",
    "    # Comprobar si el archivo tiene datos\n",
    "    if not eval_df.empty:\n",
    "        max_reward = eval_df[\"reward\"].max()\n",
    "    else:\n",
    "        max_reward = float('-inf')\n",
    "\n",
    "    # Evaluar el modelo en el entorno dado\n",
    "    mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=n_eval_episodes, return_episode_rewards=False)\n",
    "\n",
    "    # Comparar la recompensa y actualizar el archivo si es necesario\n",
    "    if mean_reward > max_reward:\n",
    "        # Registrar la nueva recompensa y timestamp en el archivo CSV\n",
    "        new_row = pd.DataFrame({\n",
    "            \"reward\": [mean_reward],\n",
    "            \"timestamp\": [datetime.now().strftime(\"%Y%m%d_%H%M%S\")]\n",
    "        })\n",
    "        new_row.to_csv(eval_file_path, mode='a', header=False, index=False)\n",
    "        result = True\n",
    "    else:\n",
    "        result = False\n",
    "    env.close()\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-24T12:24:49.828343800Z",
     "start_time": "2024-12-24T12:24:49.799340800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#aquí están los archivos del simulador y la configuración de la recompensa\n",
    "world_dir = \"/Users/jeste/Desktop/Clase/TFG/drone_tfg_juanes/simulation_package/worlds/my_frst_webots_world.wbt\"\n",
    "json_reward = \"/Users/jeste/Desktop/Clase/TFG/drone_tfg_juanes/configs/reward_package_config/motors_use.json\"\n",
    "# Define el número de entornos que se van a crear\n",
    "num_envs = 4\n",
    "\n",
    "#direcciones de dónde se guardará cada componente iportante del modelo\n",
    "model_dir = \"./models/ppomodel\"\n",
    "log_dir = \"./logs/\"\n",
    "data_collected_dir = './data_collected/'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "#los valores de learning rate y entropy coefficient\n",
    "lr = 1e-3\n",
    "ent_coef = 0.06\n",
    "\n",
    "#define los pasos totales que se usarán para entrenar al modelo en cada ciclo\n",
    "timesteps = 20480\n",
    "#define los pasos que se usrán antes de actualizar los pesos del modelo\n",
    "n_steps = 1024\n",
    "#define los paquetes de experiencia que se usarán para actualizar los pesos\n",
    "batch_size = 64\n",
    "\n",
    "#ciclos\n",
    "n_cycles = 2\n",
    "timeout_threshold = 240\n",
    "\n",
    "#creamos el entorno\n",
    "env = SubprocVecEnv([make_env() for _ in range(num_envs)])\n",
    "env = VecMonitor(env)#, filename=f'./data_collected/ppo_monitor{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-24T12:24:58.836994700Z",
     "start_time": "2024-12-24T12:24:49.819341800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/\n",
      "retrainning\n"
     ]
    }
   ],
   "source": [
    "new_logger = configure(log_dir, [\"stdout\", \"csv\"])\n",
    "callback = TrainingCallback(env=env, verbose=1)\n",
    "\n",
    "if not os.path.exists(model_dir+\".zip\"):\n",
    "    print(\"first train\")\n",
    "    model = RecurrentPPO(\n",
    "        \"MultiInputLstmPolicy\",\n",
    "        env,\n",
    "        verbose=1,                    # Si quiero ver las acciones por terminal\n",
    "        n_steps=n_steps,              # Controla el buffer de experiencias para actualizar la política\n",
    "        batch_size=batch_size,        # Tamaño del lote, separa el buffer de experiencias en paquetes de este tamaño\n",
    "        learning_rate=lr,     # Tasa de aprendizaje\n",
    "        ent_coef=ent_coef     # Coeficiente de entropía para exploración\n",
    "    )\n",
    "    model.set_logger(new_logger)\n",
    "    model.learn(total_timesteps=timesteps, callback=callback)\n",
    "    model.save(model_dir)\n",
    "\n",
    "    move_and_rename_csv(log_dir, data_collected_dir, f'ppo_data{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv')\n",
    "else:\n",
    "    print(\"retrainning\")\n",
    "    model = RecurrentPPO.load(model_dir+\".zip\", env=env)\n",
    "    model.set_logger(new_logger)\n",
    "\n",
    "    model.learning_rate = lr\n",
    "    model.ent_coef = ent_coef\n",
    "\n",
    "    model.learn(total_timesteps=timesteps, callback=callback)\n",
    "    time.sleep(5)\n",
    "    move_and_rename_csv(log_dir, data_collected_dir, f'ppo_data{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv')\n",
    "\n",
    "    if update_model(model, make_env()(), n_eval_episodes=10):\n",
    "        model.save(path=model_dir)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-12-24T12:24:58.841995500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "init = make_env()\n",
    "show_env = init()\n",
    "\n",
    "model = RecurrentPPO.load(\"models/ppomodel\", env=env)\n",
    "\n",
    "observation, _ = show_env.reset()\n",
    "\n",
    "for i in range(30):\n",
    "    action, _states = model.predict(observation, deterministic=True)\n",
    "    observation, reward, terminated, truncated, _ = show_env.step(action)\n",
    "\n",
    "    if terminated:\n",
    "        observation, _ = show_env.reset()\n",
    "\n",
    "show_env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-12-24T12:25:30.056384400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-12-24T12:25:30.057383500Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
