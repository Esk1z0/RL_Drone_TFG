{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import enviroments_package\n",
    "from enviroments_package import RemoveKeyObservationWrapper, ScaleRewardWrapper, ScaleActionWrapper, BinaryActionWrapper\n",
    "import gymnasium\n",
    "\n",
    "from stable_baselines3.common.env_util import SubprocVecEnv\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import configure\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#se crea una función para fabricar cada entorno con los wrapers correspondientes para un SubprocVecEnv\n",
    "def make_env():\n",
    "    def _init():\n",
    "        # Crea el entorno base\n",
    "        env = gymnasium.make('drone_tfg_juanes/Drone-v1', simulation_path=world_dir, reward_json_path=json_reward, no_render=False)\n",
    "\n",
    "        # Aplica los wrappers necesarios\n",
    "        env = RemoveKeyObservationWrapper(env, remove_keys=[\"camera\", \"gps\"])\n",
    "        env = ScaleRewardWrapper(env, scale_factor=0.1)\n",
    "        env = ScaleActionWrapper(env, in_low=-1, in_high=1, out_low=0, out_high=576)\n",
    "        #env = BinaryActionWrapper(env, power_level=500)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "#creamos un callback para que el entorno funcione correctamente y no haya problemas con el entorno\n",
    "class TrainingCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=1):\n",
    "        super(TrainingCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_start(self) -> None:\n",
    "        self.env.reset()\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        print(\"Entrenamiento finalizado. Cerrando el entorno...\")\n",
    "        self.env.close()\n",
    "\n",
    "#aquí creamos un archivo para mover el progress.csv que geenra cada entrenamiento a la carpeta de data_collected\n",
    "def move_and_rename_csv(src_dir, dst_dir, new_name):\n",
    "    # Definir el archivo CSV específico a buscar\n",
    "    csv_file = 'progress.csv'\n",
    "    src_path = os.path.join(src_dir, csv_file)\n",
    "\n",
    "    # Verificar si el archivo 'progress.csv' existe en el directorio de origen\n",
    "    if not os.path.exists(src_path):\n",
    "        print(\"No se encontró el archivo 'progress.csv' en el directorio de origen.\")\n",
    "        return\n",
    "\n",
    "    # Definir la ruta de destino con el nuevo nombre\n",
    "    dst_path = os.path.join(dst_dir, new_name)\n",
    "\n",
    "    # Mover y renombrar el archivo\n",
    "    shutil.copy2(src_path, dst_path)\n",
    "    print(f\"Archivo copiado y renombrado a {dst_path}\")\n",
    "\n",
    "\n",
    "#creo una función para modificar el learning rate y el entropy coefficient\n",
    "def schedule_rate(initial_value, final_value, total_cycles, current_cycle):\n",
    "    return final_value if current_cycle >= total_cycles else initial_value + (final_value - initial_value) * (current_cycle / total_cycles)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T12:15:53.035820700Z",
     "start_time": "2025-01-30T12:15:53.020734600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "def update_model(model, env, log_dir='./logs/', n_eval_episodes=10):\n",
    "    \"\"\"\n",
    "    Evalúa el rendimiento del modelo y actualiza un archivo de evaluación en CSV.\n",
    "\n",
    "    Args:\n",
    "        model: Modelo de Stable-Baselines3 a evaluar.\n",
    "        env: Entorno de entrenamiento.\n",
    "        log_dir (str): Carpeta para almacenar el archivo de evaluación.\n",
    "        n_eval_episodes (int): Número de episodios para evaluar el modelo.\n",
    "\n",
    "    Returns:\n",
    "        bool: True si el modelo supera la recompensa máxima registrada, False en caso contrario.\n",
    "    \"\"\"\n",
    "    eval_file_path = os.path.join(log_dir, \"evaluate.csv\")\n",
    "\n",
    "    # Crear el directorio y archivo si no existen\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    if not os.path.exists(eval_file_path):\n",
    "        # Crear archivo vacío con encabezado\n",
    "        pd.DataFrame(columns=[\"reward\", \"timestamp\"]).to_csv(eval_file_path, index=False)\n",
    "\n",
    "    # Leer el archivo de evaluación\n",
    "    eval_df = pd.read_csv(eval_file_path)\n",
    "\n",
    "    # Comprobar si el archivo tiene datos\n",
    "    if not eval_df.empty:\n",
    "        max_reward = eval_df[\"reward\"].max()\n",
    "    else:\n",
    "        max_reward = float('-inf')\n",
    "\n",
    "    # Evaluar el modelo en el entorno dado\n",
    "    mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=n_eval_episodes, return_episode_rewards=False)\n",
    "\n",
    "    # Comparar la recompensa y actualizar el archivo si es necesario\n",
    "    if mean_reward > max_reward:\n",
    "        # Registrar la nueva recompensa y timestamp en el archivo CSV\n",
    "        new_row = pd.DataFrame({\n",
    "            \"reward\": [mean_reward],\n",
    "            \"timestamp\": [datetime.now().strftime(\"%Y%m%d_%H%M%S\")]\n",
    "        })\n",
    "        new_row.to_csv(eval_file_path, mode='a', header=False, index=False)\n",
    "        result = True\n",
    "    else:\n",
    "        result = False\n",
    "    env.close()\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T12:15:53.057976700Z",
     "start_time": "2025-01-30T12:15:53.042820400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#aquí están los archivos del simulador y la configuración de la recompensa\n",
    "world_dir = \"/Users/jeste/Desktop/Clase/TFG/drone_tfg_juanes/simulation_package/worlds/my_frst_webots_world.wbt\"\n",
    "json_reward = \"/Users/jeste/Desktop/Clase/TFG/drone_tfg_juanes/configs/reward_package_config/motors_use.json\"\n",
    "# Define el número de entornos que se van a crear\n",
    "num_envs = 4\n",
    "\n",
    "#direcciones de dónde se guardará cada componente iportante del modelo\n",
    "model_dir = \"./models/ppomodel\"\n",
    "log_dir = \"./logs/\"\n",
    "data_collected_dir = './data_collected/'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "#los valores de learning rate y entropy coefficient\n",
    "lr = 1e-3\n",
    "ent_coef = 0.06\n",
    "\n",
    "#define los pasos totales que se usarán para entrenar al modelo en cada ciclo\n",
    "timesteps = 20480\n",
    "#define los pasos que se usrán antes de actualizar los pesos del modelo\n",
    "n_steps = 1024\n",
    "#define los paquetes de experiencia que se usarán para actualizar los pesos\n",
    "batch_size = 64\n",
    "\n",
    "#ciclos\n",
    "n_cycles = 2\n",
    "timeout_threshold = 240\n",
    "\n",
    "#creamos el entorno\n",
    "env = SubprocVecEnv([make_env() for _ in range(num_envs)])\n",
    "env = VecMonitor(env)#, filename=f'./data_collected/ppo_monitor{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T12:16:02.765783400Z",
     "start_time": "2025-01-30T12:15:53.052922900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/\n",
      "retrainning\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 14.3     |\n",
      "|    ep_rew_mean     | 7.56     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 388      |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mBrokenPipeError\u001B[0m                           Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\lib\\multiprocessing\\connection.py:317\u001B[0m, in \u001B[0;36mPipeConnection._recv_bytes\u001B[1;34m(self, maxsize)\u001B[0m\n\u001B[0;32m    316\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m--> 317\u001B[0m     nread, err \u001B[38;5;241m=\u001B[39m \u001B[43mov\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mGetOverlappedResult\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    318\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m err \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "\u001B[1;31mBrokenPipeError\u001B[0m: [WinError 109] Ha terminado la canalización",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mEOFError\u001B[0m                                  Traceback (most recent call last)",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     25\u001B[0m model\u001B[38;5;241m.\u001B[39mlearning_rate \u001B[38;5;241m=\u001B[39m lr\n\u001B[0;32m     26\u001B[0m model\u001B[38;5;241m.\u001B[39ment_coef \u001B[38;5;241m=\u001B[39m ent_coef\n\u001B[1;32m---> 28\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimesteps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     29\u001B[0m time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m5\u001B[39m)\n\u001B[0;32m     30\u001B[0m move_and_rename_csv(log_dir, data_collected_dir, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mppo_data\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdatetime\u001B[38;5;241m.\u001B[39mnow()\u001B[38;5;241m.\u001B[39mstrftime(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mm\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mH\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mM\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mS\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sb3_contrib\\ppo_recurrent\\ppo_recurrent.py:454\u001B[0m, in \u001B[0;36mRecurrentPPO.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    445\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlearn\u001B[39m(\n\u001B[0;32m    446\u001B[0m     \u001B[38;5;28mself\u001B[39m: SelfRecurrentPPO,\n\u001B[0;32m    447\u001B[0m     total_timesteps: \u001B[38;5;28mint\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    452\u001B[0m     progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    453\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SelfRecurrentPPO:\n\u001B[1;32m--> 454\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    455\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    456\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    457\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    458\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_log_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    459\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    461\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    297\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    299\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m<\u001B[39m total_timesteps:\n\u001B[1;32m--> 300\u001B[0m     continue_training \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect_rollouts\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrollout_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_rollout_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_steps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    302\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m continue_training:\n\u001B[0;32m    303\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\sb3_contrib\\ppo_recurrent\\ppo_recurrent.py:229\u001B[0m, in \u001B[0;36mRecurrentPPO.collect_rollouts\u001B[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_sde:\n\u001B[0;32m    227\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy\u001B[38;5;241m.\u001B[39mreset_noise(env\u001B[38;5;241m.\u001B[39mnum_envs)\n\u001B[1;32m--> 229\u001B[0m \u001B[43mcallback\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_rollout_start\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m lstm_states \u001B[38;5;241m=\u001B[39m deepcopy(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_last_lstm_states)\n\u001B[0;32m    233\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m n_steps \u001B[38;5;241m<\u001B[39m n_rollout_steps:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\callbacks.py:90\u001B[0m, in \u001B[0;36mBaseCallback.on_rollout_start\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     89\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mon_rollout_start\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 90\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_on_rollout_start\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36mTrainingCallback._on_rollout_start\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_on_rollout_start\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 25\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_monitor.py:70\u001B[0m, in \u001B[0;36mVecMonitor.reset\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     69\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m VecEnvObs:\n\u001B[1;32m---> 70\u001B[0m     obs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvenv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     71\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepisode_returns \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_envs, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepisode_lengths \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_envs, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mint32)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\subproc_vec_env.py:137\u001B[0m, in \u001B[0;36mSubprocVecEnv.reset\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m env_idx, remote \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mremotes):\n\u001B[0;32m    136\u001B[0m     remote\u001B[38;5;241m.\u001B[39msend((\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreset\u001B[39m\u001B[38;5;124m\"\u001B[39m, (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_seeds[env_idx], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_options[env_idx])))\n\u001B[1;32m--> 137\u001B[0m results \u001B[38;5;241m=\u001B[39m [remote\u001B[38;5;241m.\u001B[39mrecv() \u001B[38;5;28;01mfor\u001B[39;00m remote \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mremotes]\n\u001B[0;32m    138\u001B[0m obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreset_infos \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mresults)  \u001B[38;5;66;03m# type: ignore[assignment]\u001B[39;00m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;66;03m# Seeds and options are only used once\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\subproc_vec_env.py:137\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m env_idx, remote \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mremotes):\n\u001B[0;32m    136\u001B[0m     remote\u001B[38;5;241m.\u001B[39msend((\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreset\u001B[39m\u001B[38;5;124m\"\u001B[39m, (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_seeds[env_idx], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_options[env_idx])))\n\u001B[1;32m--> 137\u001B[0m results \u001B[38;5;241m=\u001B[39m [\u001B[43mremote\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m remote \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mremotes]\n\u001B[0;32m    138\u001B[0m obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreset_infos \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mresults)  \u001B[38;5;66;03m# type: ignore[assignment]\u001B[39;00m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;66;03m# Seeds and options are only used once\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\multiprocessing\\connection.py:255\u001B[0m, in \u001B[0;36m_ConnectionBase.recv\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    253\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_closed()\n\u001B[0;32m    254\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_readable()\n\u001B[1;32m--> 255\u001B[0m buf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_recv_bytes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    256\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _ForkingPickler\u001B[38;5;241m.\u001B[39mloads(buf\u001B[38;5;241m.\u001B[39mgetbuffer())\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\multiprocessing\\connection.py:326\u001B[0m, in \u001B[0;36mPipeConnection._recv_bytes\u001B[1;34m(self, maxsize)\u001B[0m\n\u001B[0;32m    324\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    325\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwinerror \u001B[38;5;241m==\u001B[39m _winapi\u001B[38;5;241m.\u001B[39mERROR_BROKEN_PIPE:\n\u001B[1;32m--> 326\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEOFError\u001B[39;00m\n\u001B[0;32m    327\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[1;31mEOFError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "new_logger = configure(log_dir, [\"stdout\", \"csv\"])\n",
    "callback = TrainingCallback(env=env, verbose=1)\n",
    "\n",
    "if not os.path.exists(model_dir+\".zip\"):\n",
    "    print(\"first train\")\n",
    "    model = RecurrentPPO(\n",
    "        \"MultiInputLstmPolicy\",\n",
    "        env,\n",
    "        verbose=1,                    # Si quiero ver las acciones por terminal\n",
    "        n_steps=n_steps,              # Controla el buffer de experiencias para actualizar la política\n",
    "        batch_size=batch_size,        # Tamaño del lote, separa el buffer de experiencias en paquetes de este tamaño\n",
    "        learning_rate=lr,     # Tasa de aprendizaje\n",
    "        ent_coef=ent_coef     # Coeficiente de entropía para exploración\n",
    "    )\n",
    "    model.set_logger(new_logger)\n",
    "    model.learn(total_timesteps=timesteps, callback=callback)\n",
    "    model.save(model_dir)\n",
    "\n",
    "    move_and_rename_csv(log_dir, data_collected_dir, f'ppo_data{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv')\n",
    "else:\n",
    "    print(\"retrainning\")\n",
    "    model = RecurrentPPO.load(model_dir+\".zip\", env=env)\n",
    "    model.set_logger(new_logger)\n",
    "\n",
    "    model.learning_rate = lr\n",
    "    model.ent_coef = ent_coef\n",
    "\n",
    "    model.learn(total_timesteps=timesteps, callback=callback)\n",
    "    time.sleep(5)\n",
    "    move_and_rename_csv(log_dir, data_collected_dir, f'ppo_data{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv')\n",
    "\n",
    "    if update_model(model, make_env()(), n_eval_episodes=10):\n",
    "        model.save(path=model_dir)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-30T12:23:45.841092900Z",
     "start_time": "2025-01-30T12:16:02.773785Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "init = make_env()\n",
    "show_env = init()\n",
    "\n",
    "model = RecurrentPPO.load(\"models/ppomodel\", env=env)\n",
    "\n",
    "observation, _ = show_env.reset()\n",
    "\n",
    "for i in range(30):\n",
    "    action, _states = model.predict(observation, deterministic=True)\n",
    "    observation, reward, terminated, truncated, _ = show_env.step(action)\n",
    "\n",
    "    if terminated:\n",
    "        observation, _ = show_env.reset()\n",
    "\n",
    "show_env.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
