{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import enviroments_package\n",
    "import gymnasium\n",
    "\n",
    "world_dir = \"/Users/jeste/Desktop/Clase/TFG/drone_tfg_juanes/simulation_package/worlds/my_frst_webots_world.wbt\"\n",
    "json_take_off = \"/Users/jeste/Desktop/Clase/TFG/drone_tfg_juanes/configs/reward_package_config/takeoff.json\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:53:02.571151500Z",
     "start_time": "2024-11-05T16:53:02.328150500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "from stable_baselines3.common.env_util import SubprocVecEnv\n",
    "from enviroments_package import RemoveKeyObservationWrapper, ScaleRewardWrapper, ScaleActionWrapper\n",
    "\n",
    "# Define el número de entornos que quieres crear\n",
    "num_envs = 4  # Cambia este valor según tus necesidades\n",
    "\n",
    "# Define la función make_env\n",
    "def make_env():\n",
    "    def _init():\n",
    "        # Crea el entorno base\n",
    "        env = gymnasium.make('drone_tfg_juanes/Drone-v1', simulation_path=world_dir, reward_json_path=json_take_off, no_render=False)\n",
    "\n",
    "        # Aplica los wrappers necesarios\n",
    "        env = RemoveKeyObservationWrapper(env, remove_keys=[\"camera\", \"gps\"])\n",
    "        env = ScaleRewardWrapper(env, scale_factor=0.1)\n",
    "        env = ScaleActionWrapper(env, in_low=0, in_high=2, out_low=0, out_high=576)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "# Usa make_vec_env para crear un DummyVecEnv con los entornos creados por make_env\n",
    "env = SubprocVecEnv([make_env() for _ in range(num_envs)])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:53:11.293210900Z",
     "start_time": "2024-11-05T16:53:02.345150100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "class TrainingCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=1):\n",
    "        super(TrainingCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_start(self) -> None:\n",
    "        self.env.reset()\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        # Este método se llama al final del entrenamiento\n",
    "        print(\"Entrenamiento finalizado. Cerrando el entorno...\")\n",
    "        self.env.close()  # Cerrar el entorno para evitar problemas\n",
    "\n",
    "\n",
    "# Usar el callback al entrenar\n",
    "\n",
    "callback = TrainingCallback(env=env)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T16:53:11.309210700Z",
     "start_time": "2024-11-05T16:53:11.296212300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/ppo_training/\n",
      "Using cuda device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 26   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 78   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 24           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 166          |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038696905 |\n",
      "|    clip_fraction        | 0.0373       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.73        |\n",
      "|    explained_variance   | 5.25e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.28         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | 0.00125      |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 8.37         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 24          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 253         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008994488 |\n",
      "|    clip_fraction        | 0.045       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.78       |\n",
      "|    explained_variance   | 0.000392    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.71        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00283    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 16.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 24          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 339         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003150383 |\n",
      "|    clip_fraction        | 0.0259      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.85       |\n",
      "|    explained_variance   | -0.00028    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.06        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.000524   |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 22.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 23           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 427          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039851638 |\n",
      "|    clip_fraction        | 0.0419       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.93        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.1         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    std                  | 1.08         |\n",
      "|    value_loss           | 28.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 23           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 513          |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029187223 |\n",
      "|    clip_fraction        | 0.0363       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.02        |\n",
      "|    explained_variance   | 0.00115      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.4         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.000441    |\n",
      "|    std                  | 1.1          |\n",
      "|    value_loss           | 39.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 23           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 599          |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075982576 |\n",
      "|    clip_fraction        | 0.0458       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.11        |\n",
      "|    explained_variance   | 5.25e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.2         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | 0.000181     |\n",
      "|    std                  | 1.12         |\n",
      "|    value_loss           | 38.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 686         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003421573 |\n",
      "|    clip_fraction        | 0.0253      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.17       |\n",
      "|    explained_variance   | -0.00086    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.02        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.000396   |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 42.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 23           |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 776          |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067417324 |\n",
      "|    clip_fraction        | 0.0732       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.24        |\n",
      "|    explained_variance   | -0.0286      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.9         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00235     |\n",
      "|    std                  | 1.16         |\n",
      "|    value_loss           | 52.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 861         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005491259 |\n",
      "|    clip_fraction        | 0.0454      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.32       |\n",
      "|    explained_variance   | -0.0256     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.6        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00109    |\n",
      "|    std                  | 1.19        |\n",
      "|    value_loss           | 53.8        |\n",
      "-----------------------------------------\n",
      "Entrenamiento finalizado. Cerrando el entorno...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "timesteps = 20480\n",
    "log_dir = \"./logs/ppo_training/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Configurar el logger con salida en csv, log, stdout y tensorboard\n",
    "new_logger = configure(log_dir, [\"stdout\", \"csv\", \"log\", \"tensorboard\"])\n",
    "\n",
    "# Verificar si el archivo ppomodel existe en la carpeta ./models/\n",
    "if not os.path.exists('./models/ppomodel'):\n",
    "    # Crear y entrenar el modelo solo si el archivo no existe\n",
    "    model = PPO(\n",
    "        \"MultiInputPolicy\",\n",
    "        env,\n",
    "        verbose=1,\n",
    "        n_steps=512,        # Tamaño típico, controla el buffer de experiencias para actualizar la política\n",
    "        batch_size=32,      # Tamaño del lote, normalmente 64 o 128\n",
    "        learning_rate=3e-4, # Tasa de aprendizaje típica en PPO\n",
    "        ent_coef=0.05       # Coeficiente de entropía para exploración, ajuste común en PPO\n",
    "    )\n",
    "    model.set_logger(new_logger)  # Asignar el nuevo logger al modelo\n",
    "    model.learn(total_timesteps=timesteps, callback=callback)\n",
    "    model.save('./models/ppomodel')  # Guardar el modelo para futuras ejecuciones\n",
    "else:\n",
    "    # Cargar el modelo guardado\n",
    "    model = PPO.load(\"./models/ppomodel\", env=env)\n",
    "    model.set_logger(new_logger)  # Asignar el nuevo logger al modelo cargado\n",
    "    # Continuar el entrenamiento desde el último punto\n",
    "    model.learn(total_timesteps=timesteps, callback=callback)  # Ajusta los timesteps si quieres más entrenamiento\n",
    "    model.save(path=\"./models/ppomodel\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T17:07:46.765398600Z",
     "start_time": "2024-11-05T16:53:11.313211900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo copiado y renombrado a ./data_collected/ppo20241105_180746.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def move_and_rename_csv(src_dir, dst_dir, new_name):\n",
    "    # Buscar el archivo CSV en el directorio fuente\n",
    "    csv_files = [f for f in os.listdir(src_dir) if f.endswith('.csv')]\n",
    "\n",
    "    # Verificar si hay algún archivo CSV en el directorio de origen\n",
    "    if not csv_files:\n",
    "        print(\"No se encontró ningún archivo CSV en el directorio de origen.\")\n",
    "        return\n",
    "\n",
    "    # Tomar el primer archivo CSV encontrado\n",
    "    csv_file = csv_files[0]\n",
    "    src_path = os.path.join(src_dir, csv_file)\n",
    "    dst_path = os.path.join(dst_dir, new_name)\n",
    "\n",
    "    # Mover y renombrar el archivo\n",
    "    shutil.copy2(src_path, dst_path)\n",
    "    print(f\"Archivo copiado y renombrado a {dst_path}\")\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "src_directory = log_dir\n",
    "dst_directory = './data_collected/'  # Cambia por la ruta de tu carpeta de destino\n",
    "new_filename = f'ppo{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'  # Cambia por el nombre deseado del archivo en el destino\n",
    "\n",
    "move_and_rename_csv(src_directory, dst_directory, new_filename)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T17:07:46.812241800Z",
     "start_time": "2024-11-05T17:07:46.765398600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "observation, _ = env.reset()\n",
    "\n",
    "for i in range(100):  # Cambia el número de pasos según quieras observar\n",
    "    action, _states = model.predict(observation)  # Usa deterministic=True para ver la política aprendida\n",
    "    observation, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "    if terminated:\n",
    "        observation, _ = env.reset()  # Reiniciar el entorno al final del episodio\n",
    "\n",
    "# Cerrar el entorno después de la prueba\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T17:07:46.812241800Z",
     "start_time": "2024-11-05T17:07:46.796619700Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
